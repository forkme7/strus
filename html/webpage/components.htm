<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 2.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en">
<head>
	<link rel="icon" type="image/ico" href="images/strus.ico" />
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="Description of the components of strus, a collection of C++ libraries for building a full-text search engine." />
	<meta name="keywords" content="fulltext search engine C++" />
	<meta name="author" content="Patrick Frey &lt;patrickpfrey (a) yahoo (dt) com&gt;" />
	<link rel="stylesheet" type="text/css" href="text-profile.css" title="Text Profile" media="all" />
	<title>Components of strus</title>
</head>

<body>
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-63809026-9', 'auto');
  ga('send', 'pageview');

</script><div id="wrap">
	<div id="content">
		<h1>Strus components</h1>
		<h2>Introducing the components of strus</h2>
		<p>
		This section introduces the components of strus.
		</p>
		<h2>Key/value store database</h2>
		<p>A key/value store database stores blocks of data for fast retrieval by their key.
		The database is separated as own component to allow competitive solutions
		for various architectures with different requirements implemented by experts
		for this topic. The key/value store database has to implement an upper bound
		seek on keys to support fast merging operations needed by the logical storage.
		Currently there exists an implementation based on LevelDB
		(In fact, LevelDB was the main stimulus for me to write a search engine.
		I looked at it and noticed: "<i>Heureka ! It has an upper bound seek. With this
		I can write a search engine !</i>").
		</p>
		<h3>Storage</h3>
		<p> The storage provides interfaces to define the units to store
		for retrieval and presentation of the search result. It allows you to
		define documents as numbered lists of atomic terms, content, attributes
		and meta data. For every document you can define user rights that restrict 
		access to a document to defined users. The storage groups these 
		definitions into blocks and tables stored for fast access in underlying 
		the key/value store database.
		</p>
		<h3>Query evaluation</h3>
		<p>The query evaluation combines the occurrencies of search terms according 
		to a given query to higher level expressions, ranks a set of selected documents
		according some defined weighting schemes and returns
		a list of documents with named attributes as result.
		Query evaluation is defined with the help of functions of two different types:</p>
		<h4>Weighting</h4>
		<p>Weighting accumulates a value as the weight of a document based on
		a retrieval scheme (e.g. BM25, tf-idf, proximity weighting, etc.).
		and the occurrencies of expressions in this document.
		</p>
		<h4>Summarization</h4>
		<p>Summarization extracts content elements, attributes or meta data from a
		matching document. As result summarizers return a set of weighted key
		value pairs for the presentation of the result. Summarization can be
		used for showing properties of the result to a user as well as for
		exraction of data for feature selection for another iteration of
		query evaluation in the background
		(<a href="http://en.wikipedia.org/wiki/Relevance_feedback">relevance feedback</a>).
		</p>
		<h2>Associated components of strus</h2>
		<p>For feeding a search engine there are some components needed that are not part
		of the core.
		<h3>Analyzer</h3>
		<p> The analyzer (also called indexer in other information retrieval engines)
			exists as a project, but it is not interlinked
			in an intrusive way with the strus core. The strusAnalyzer provides
			segmentation, tokenization and normalization to get the atomic terms
			to insert into the storage and to tokenize and normalize phrases of
			the query accordingly. The analyzer uses the following components
			to do its job:
		</p>
		<h4>Segmenter</h4>
		<p>The segmented splits a document of a certain format (XML,JSON,etc.) into content 
		chunks defined by selection expressions.
		Currently there exists only an implementation for XML based on the <a href="textwolf.net">textwolf</a>
		library using <a href="http://en.wikipedia.org/wiki/XPath#Abbreviated_syntax">abbreviated syntax of XPath</a>
		as selection language.
		</p>
		<h4>Tokenizer</h4>
		<p>The tokenizer splits a segment or alternatively the join of all segments 
		of a certain type into tokens. The tokens a referencing elements in the 
		segments without modification.
		</p>
		<h4>Normalizer</h4>
		<p>The normalizer maps a token to a term to be inserted into the storage.
		</p>
		<h2>Expandability of strus</h2>
		<p>Strus offers various interfaces to hook in. The project strusModule provides a
		mechanism to load functions extending capabilities of the storage or the analyzer
		of strus.
		<h3>strus core</h3>
		<p>You can extend the strus core with own dynamically loadable modules with
		functions written in C++:
		<ol>
		<h4>Iterator join operators</h4>
		<p>You can define your own functions that create an iterator on postings representing 
		the result of an n-ary join of iterators on postings.</p>
		<h4>Weighting functions</h4>
		<p>You can define your own document weighting functions used for ranking.</p>
		<h4>Summarizers</h4>
		<p>You can define your own summarization functions used for attributing the results.</p>
		<h3>strus analyzer</h3>
		<p>You can extend the strus analyzer with own dynamically loadable modules with
		functions written in C++:
		<ol>
		<h4>Segmenters</h4>
		<p>You can define your own segmenters for the document formats you need to process.</p>
		<h4>Tokenizer</h4>
		<p>You can define your own tokenizers splitting the document segments into tokens.</p>
		<h4>Normalizer</h4>
		<p>You can define your own normalizer functions to produce the retrievable items from 
		the document tokens for the storage and the query.</p>

		<h2>What is still missing in strus</h2>
		<h3>Documentation</h3>
		<p>The documentation of strus and its associated components is still poor. I am currently
		working hard on it every day.</p>

		<h3>Orchestration of a distributed index</h3>
		<p>The ability of distributing the search index and running <i>strus</i>
		as server nodes in a cluster without too much configuration and 
		administration effort is crucial for implementing a competitive search. <br/>
		<i>Strus</i> has interfaces to populate and receive statistics, that have to be shared
		in order to make ranking weights of different search engine nodes comparable. 
		<i>Strus</i> is built to take these statistics into account for ranking. 
		<i>strus</i> also has the interfaces to populate its statistics after startup 
		and all changes in its index during runtime.<br/>
		But the messaging to exchange this data and the orchestration
		of its nodes is not implemented yet.
		</p>

		<h2>What is not part of strus</h2>
		<p>Several parts are not a subject for strus. 
		Here follows a list of parts you may miss and have to find elsewhere.
		</p>
		<h3>Crawler</h3>
		<p> A crawler (also called robot) that searches for documents in the the internet 
		or an intranet to perform the input or update operations of the search index <b>is not</b>
		part of strus. There exist sophisticated solutions for different classes of document
		collections. In the strusUtilities project there exists a program that is able to insert
		all files a directory of a filesystem, but not more.
		</p>
		<h3>Mapping of hierarchical ACL trees</h3>
		<p>In strus user rights are attached to each document for each user allowed to see
		the document. ACLs are usually defined hierarchically with exclusion and inclusion
		rules defined for a node and its descendants.<br\>
		For strus you have to calculate the transitive cover of all positively
		declared user rights and assign them to each document. In this model updates of
		user rights are awkward, but taking them into account for retrieval is fast.</p>
	</div>
</div>
</body>
</html>

